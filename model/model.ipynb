{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross-modal Architecture models.\\n\\nCode based on the implementation of \"Collaborative Experts\":\\nhttps://github.com/albanie/collaborative-experts\\n\\nCode based on the implementation of \"Mixture of Embedding Experts\":\\nhttps://github.com/antoine77340/Mixture-of-Embedding-Experts\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2020 Valentin Gabeur\n",
    "# Copyright 2020 Samuel Albanie, Yang Liu and Arsha Nagrani\n",
    "# Copyright 2018 Antoine Miech All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Cross-modal Architecture models.\n",
    "\n",
    "Code based on the implementation of \"Collaborative Experts\":\n",
    "https://github.com/albanie/collaborative-experts\n",
    "\n",
    "Code based on the implementation of \"Mixture of Embedding Experts\":\n",
    "https://github.com/antoine77340/Mixture-of-Embedding-Experts\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8182f3c9485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'base'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import re\n",
    "import types\n",
    "\n",
    "from base import BaseModel\n",
    "from model.bert import BertModel\n",
    "from model.lstm import LSTMModel\n",
    "from model.net_vlad import NetVLAD\n",
    "from model.txt_embeddings import TxtEmbeddings\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_bert import BertModel as TxtBertModel\n",
    "from utils.util import get_len_sequences\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CENet(BaseModel):\n",
    "  \"\"\"Whole cross-modal architecture.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               l2renorm,\n",
    "               expert_dims,\n",
    "               tokenizer,\n",
    "               keep_missing_modalities,\n",
    "               test_caption_mode,\n",
    "               freeze_weights=False,\n",
    "               mimic_ce_dims=False,\n",
    "               concat_experts=False,\n",
    "               concat_mix_experts=False,\n",
    "               use_experts='origfeat',\n",
    "               txt_inp=None,\n",
    "               txt_agg=None,\n",
    "               txt_pro=None,\n",
    "               txt_wgh=None,\n",
    "               vid_inp=None,\n",
    "               vid_cont=None,\n",
    "               vid_wgh=None,\n",
    "               pos_enc=None,\n",
    "               out_tok=None,\n",
    "               use_mask='nomask',\n",
    "               same_dim=512,\n",
    "               vid_bert_params=None,\n",
    "               txt_bert_params=None,\n",
    "               agg_dims=None,\n",
    "               normalize_experts=True):\n",
    "    super().__init__()\n",
    "\n",
    "    self.sanity_checks = False\n",
    "    modalities = list(expert_dims.keys())\n",
    "    self.expert_dims = expert_dims\n",
    "    self.modalities = modalities\n",
    "    logger.debug(self.modalities)\n",
    "    self.mimic_ce_dims = mimic_ce_dims\n",
    "    self.concat_experts = concat_experts\n",
    "    self.concat_mix_experts = concat_mix_experts\n",
    "    self.test_caption_mode = test_caption_mode\n",
    "    self.freeze_weights = freeze_weights\n",
    "    self.use_experts = use_experts\n",
    "    self.use_mask = use_mask\n",
    "    self.keep_missing_modalities = keep_missing_modalities\n",
    "    self.l2renorm = l2renorm\n",
    "    self.same_dim = same_dim\n",
    "    self.txt_inp = txt_inp\n",
    "    self.txt_agg = txt_agg\n",
    "    self.txt_pro = txt_pro\n",
    "    self.txt_wgh = txt_wgh\n",
    "    self.vid_inp = vid_inp\n",
    "    self.vid_cont = vid_cont\n",
    "    self.vid_wgh = vid_wgh\n",
    "    self.pos_enc = pos_enc\n",
    "    self.out_tok = out_tok\n",
    "    self.vid_bert_params = vid_bert_params\n",
    "    self.normalize_experts = normalize_experts\n",
    "\n",
    "    self.video_dim_reduce = nn.ModuleDict()\n",
    "    for mod in self.modalities:\n",
    "      in_dim = expert_dims[mod]['dim']\n",
    "      if self.vid_inp in ['agg', 'both', 'all', 'temp']:\n",
    "        self.video_dim_reduce[mod] = ReduceDim(in_dim, same_dim)\n",
    "\n",
    "    if self.vid_cont == 'coll':\n",
    "      self.g_reason_1 = nn.Linear(same_dim * 2, same_dim)\n",
    "      dout_prob = vid_bert_params['hidden_dropout_prob']\n",
    "      self.coll_g_dropout = nn.Dropout(dout_prob)\n",
    "      self.g_reason_2 = nn.Linear(same_dim, same_dim)\n",
    "\n",
    "      self.f_reason_1 = nn.Linear(same_dim, same_dim)\n",
    "      self.coll_f_dropout = nn.Dropout(dout_prob)\n",
    "      self.f_reason_2 = nn.Linear(same_dim, same_dim)\n",
    "      self.f_reason_3 = nn.Linear(same_dim, same_dim)\n",
    "      self.batch_norm_g1 = nn.BatchNorm1d(same_dim)\n",
    "      self.batch_norm_g2 = nn.BatchNorm1d(same_dim)\n",
    "\n",
    "      self.batch_norm_f1 = nn.BatchNorm1d(same_dim)\n",
    "      self.batch_norm_f2 = nn.BatchNorm1d(same_dim)\n",
    "\n",
    "      self.video_GU = nn.ModuleDict()\n",
    "      for mod in self.modalities:\n",
    "        self.video_GU[mod] = GatedEmbeddingUnitReasoning(same_dim)\n",
    "\n",
    "    # If Bert architecture is employed for video\n",
    "    elif self.vid_cont == 'bert':\n",
    "      vid_bert_config = types.SimpleNamespace(**vid_bert_params)\n",
    "      self.vid_bert = BertModel(vid_bert_config)\n",
    "\n",
    "    elif self.vid_cont == 'none':\n",
    "      pass\n",
    "\n",
    "    if self.txt_agg[:4] in ['bert']:\n",
    "      z = re.match(r'bert([a-z]{3})(\\d*)(\\D*)', txt_agg)\n",
    "      assert z\n",
    "      state = z.groups()[0]\n",
    "      freeze_until = z.groups()[1]\n",
    "\n",
    "      # Post aggregation: Use [CLS] token (\"cls\") or aggregate all tokens\n",
    "      # (mxp, mnp)\n",
    "      if z.groups()[2] and z.groups()[2] != 'cls':\n",
    "        self.post_agg = z.groups()[2]\n",
    "      else:\n",
    "        self.post_agg = 'cls'\n",
    "\n",
    "      if state in ['ftn', 'frz']:\n",
    "        # State is finetune or frozen, we use a pretrained bert model\n",
    "        txt_bert_config = 'bert-base-cased'\n",
    "\n",
    "        # Overwrite config\n",
    "        if txt_bert_params is None:\n",
    "          dout_prob = vid_bert_params['hidden_dropout_prob']\n",
    "          txt_bert_params = {\n",
    "              'hidden_dropout_prob': dout_prob,\n",
    "              'attention_probs_dropout_prob': dout_prob,\n",
    "          }\n",
    "        self.txt_bert = TxtBertModel.from_pretrained(txt_bert_config,\n",
    "                                                     **txt_bert_params)\n",
    "\n",
    "        if state == 'frz':\n",
    "          if freeze_until:\n",
    "            # Freeze only certain layers\n",
    "            freeze_until = int(freeze_until)\n",
    "            logger.debug('Freezing text bert until layer %d excluded',\n",
    "                         freeze_until)\n",
    "            # Freeze net until given layer\n",
    "            for name, param in self.txt_bert.named_parameters():\n",
    "              module = name.split('.')[0]\n",
    "              if name.split('.')[2].isdigit():\n",
    "                layer_nb = int(name.split('.')[2])\n",
    "              else:\n",
    "                continue\n",
    "              if module == 'encoder' and layer_nb in range(freeze_until):\n",
    "                param.requires_grad = False\n",
    "                logger.debug(name)\n",
    "          else:\n",
    "            # Freeze the whole model\n",
    "            for name, param in self.txt_bert.named_parameters():\n",
    "              module = name.split('.')[0]\n",
    "              if module == 'encoder':\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "          assert not freeze_until\n",
    "\n",
    "      if self.txt_inp == 'bertfrz':\n",
    "        # Freeze model\n",
    "        for param in self.txt_bert.embeddings.parameters():\n",
    "          param.requires_grad = False\n",
    "      elif self.txt_inp not in ['bertftn']:\n",
    "        logger.error('Wrong parameter for the text encoder')\n",
    "      text_dim = self.txt_bert.config.hidden_size\n",
    "    elif self.txt_agg in ['vlad', 'mxp', 'mnp', 'lstm']:\n",
    "      # Need to get text embeddings\n",
    "      if self.txt_inp == 'bertfrz':\n",
    "        ckpt = 'data/word_embeddings/bert/ckpt_from_huggingface.pth'\n",
    "        self.word_embeddings = TxtEmbeddings(ckpt=ckpt, freeze=True)\n",
    "      elif self.txt_inp == 'bertftn':\n",
    "        ckpt = 'data/word_embeddings/bert/ckpt_from_huggingface.pth'\n",
    "        self.word_embeddings = TxtEmbeddings(ckpt=ckpt)\n",
    "      elif self.txt_inp == 'bertscr':\n",
    "        vocab_size = 28996\n",
    "        emb_dim = 768\n",
    "        self.word_embeddings = TxtEmbeddings(vocab_size, emb_dim)\n",
    "      else:\n",
    "        self.word_embeddings = tokenizer.we_model\n",
    "      emb_dim = self.word_embeddings.text_dim\n",
    "\n",
    "      if self.txt_agg == 'vlad':\n",
    "        self.text_pooling = NetVLAD(\n",
    "            feature_size=emb_dim,\n",
    "            cluster_size=28,\n",
    "        )\n",
    "        text_dim = self.text_pooling.out_dim\n",
    "      elif self.txt_agg == 'mxp':\n",
    "        text_dim = emb_dim\n",
    "      elif self.txt_agg == 'lstm':\n",
    "        input_dim = self.word_embeddings.text_dim\n",
    "        hidden_dim = 512\n",
    "        layer_dim = 1\n",
    "        output_dim = hidden_dim\n",
    "        self.text_pooling = LSTMModel(input_dim, hidden_dim, layer_dim,\n",
    "                                      output_dim)\n",
    "        text_dim = output_dim\n",
    "\n",
    "    self.text_GU = nn.ModuleDict()\n",
    "    for mod in self.modalities:\n",
    "      if self.txt_pro == 'gbn':\n",
    "        self.text_GU[mod] = GatedEmbeddingUnit(text_dim,\n",
    "                                               same_dim,\n",
    "                                               use_bn=True,\n",
    "                                               normalize=self.normalize_experts)\n",
    "      elif self.txt_pro == 'gem':\n",
    "        self.text_GU[mod] = GatedEmbeddingUnit(text_dim,\n",
    "                                               same_dim,\n",
    "                                               use_bn=False,\n",
    "                                               normalize=self.normalize_experts)\n",
    "      elif self.txt_pro == 'lin':\n",
    "        self.text_GU[mod] = ReduceDim(text_dim, same_dim)\n",
    "\n",
    "    # Weightening of each modality similarity\n",
    "    if self.txt_wgh == 'emb':\n",
    "      self.moe_fc_txt = nn.ModuleDict()\n",
    "      dout_prob = txt_bert_params['hidden_dropout_prob']\n",
    "      self.moe_txt_dropout = nn.Dropout(dout_prob)\n",
    "      for mod in self.modalities:\n",
    "        self.moe_fc_txt[mod] = nn.Linear(text_dim, 1)\n",
    "    if self.vid_wgh == 'emb':\n",
    "      self.moe_fc_vid = nn.ModuleDict()\n",
    "      dout_prob = vid_bert_params['hidden_dropout_prob']\n",
    "      self.moe_vid_dropout = nn.Dropout(dout_prob)\n",
    "      for mod in self.modalities:\n",
    "        self.moe_fc_vid[mod] = nn.Linear(self.same_dim, 1)\n",
    "\n",
    "    self.debug_dataloader = False\n",
    "    if self.debug_dataloader:\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "  def compute_weights_from_emb(self, embd):\n",
    "    # Compute the modality weights given an embedding\n",
    "\n",
    "    # vid emb\n",
    "    if len(embd.size()) == 2:\n",
    "      embd = self.moe_vid_dropout(embd)\n",
    "      moe_weights = th.cat(\n",
    "          [self.moe_fc_vid[mod](embd) for mod in self.modalities], dim=-1)\n",
    "      moe_weights = F.softmax(moe_weights, dim=1)\n",
    "\n",
    "    # text emb\n",
    "    elif len(embd.size()) == 3:\n",
    "      embd = self.moe_txt_dropout(embd)\n",
    "      b, k, d = embd.size()\n",
    "      m = len(self.modalities)\n",
    "      embd = embd.view(b * k, d)\n",
    "      moe_weights = th.cat(\n",
    "          [self.moe_fc_txt[mod](embd) for mod in self.modalities], dim=-1)\n",
    "      moe_weights = F.softmax(moe_weights, dim=1)\n",
    "      moe_weights = moe_weights.view(b, k, m)\n",
    "\n",
    "    return moe_weights\n",
    "\n",
    "  def compute_weights_from_norm(self, embds):\n",
    "    # Compute the modality weights according to their norm\n",
    "\n",
    "    device = embds[self.modalities[0]].device\n",
    "    # vid emb\n",
    "    if len(embds[self.modalities[0]].size()) == 2:\n",
    "      b, d = embds[self.modalities[0]].size()\n",
    "\n",
    "    # text emb\n",
    "    elif len(embds[self.modalities[0]].size()) == 3:\n",
    "      b, k, d = embds[self.modalities[0]].size()\n",
    "      for idx, mod in self.modalities:\n",
    "        embds[mod] = embds[mod].view(b * k, d)\n",
    "      b = b * k\n",
    "\n",
    "    m = len(self.modalities)\n",
    "    norm_embd = th.zeros(b, m).to(device)\n",
    "    for idx, mod in enumerate(self.modalities):\n",
    "      norm_embd[:, idx] = th.norm(embds[mod], p=2, dim=1)\n",
    "\n",
    "    sum_norm = th.sum(norm_embd, dim=1)  # b\n",
    "    sum_norm = sum_norm.unsqueeze(1)  # b x 1\n",
    "\n",
    "    weights = th.div(norm_embd, sum_norm)\n",
    "\n",
    "    return weights\n",
    "\n",
    "  def forward(self,\n",
    "              token_ids,\n",
    "              features,\n",
    "              features_t,\n",
    "              features_ind,\n",
    "              features_avgpool,\n",
    "              features_maxpool,\n",
    "              query_masks,\n",
    "              out='conf',\n",
    "              device=None,\n",
    "              debug=None):\n",
    "\n",
    "    self.device = device\n",
    "    experts_feats = features\n",
    "    experts_feats_t = features_t\n",
    "    experts_feats_ind = features_ind\n",
    "    ind = {}\n",
    "    for mod in self.modalities:\n",
    "      ind[mod] = th.max(experts_feats_ind[mod], 1)[0]\n",
    "    pooled_experts = {}\n",
    "\n",
    "    for _, mod in enumerate(self.modalities):\n",
    "      pooled_experts[f'{mod}_avgpool'] = features_avgpool[mod]\n",
    "      pooled_experts[f'{mod}_maxpool'] = features_maxpool[mod]\n",
    "\n",
    "    # Notation: B = batch size, M = number of modalities\n",
    "\n",
    "    # Output experts\n",
    "    experts = collections.OrderedDict()\n",
    "\n",
    "    # Pass text embeddings through gated units\n",
    "    text_embd = {}\n",
    "\n",
    "    # Unroll repeated captions into present minibatch\n",
    "    b, captions_per_video, max_text_words, feat_dim = token_ids.size()\n",
    "    m = len(self.modalities)\n",
    "\n",
    "    if self.txt_agg[:4] in ['bert']:\n",
    "      token_ids = token_ids.view(b * captions_per_video, max_text_words,\n",
    "                                 feat_dim)\n",
    "\n",
    "      input_ids_list = []\n",
    "      token_type_ids_list = []  # Modality id\n",
    "      position_ids_list = []  # Position\n",
    "      attention_mask_list = []  # Valid token or not\n",
    "\n",
    "      ids_size = (b * captions_per_video,)\n",
    "\n",
    "      for pos_id in range(max_text_words):\n",
    "        input_ids_list.append(token_ids[:, pos_id, 0].to(dtype=th.long))\n",
    "        token_type_ids_list.append(th.full(ids_size, 0, dtype=th.long))\n",
    "        position_ids_list.append(th.full(ids_size, pos_id, dtype=th.long))\n",
    "        attention_mask_list.append(token_ids[:, pos_id, 1].to(dtype=th.long))\n",
    "\n",
    "      input_ids = th.stack(input_ids_list, dim=1).to(device)\n",
    "      token_type_ids = th.stack(token_type_ids_list, dim=1).to(device)\n",
    "      position_ids = th.stack(position_ids_list, dim=1).to(device)\n",
    "      attention_mask = th.stack(attention_mask_list, dim=1).to(device)\n",
    "\n",
    "      txt_bert_output = self.txt_bert(input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      position_ids=position_ids,\n",
    "                                      head_mask=None)\n",
    "      last_layer = txt_bert_output[0]\n",
    "\n",
    "      if self.post_agg == 'cls':\n",
    "        text = last_layer[:, 0]\n",
    "\n",
    "      elif self.post_agg == 'mxp':\n",
    "        embeddings = last_layer[:, 1:]\n",
    "        text, _ = th.max(embeddings, 1)\n",
    "\n",
    "      elif self.post_agg == 'mnp':\n",
    "        embeddings = last_layer[:, 1:]\n",
    "        text = th.mean(embeddings, 1)\n",
    "\n",
    "    elif self.txt_agg in ['vlad', 'mxp', 'mnp', 'lstm']:\n",
    "      # Need to get text embeddings\n",
    "      token_ids = token_ids.view(b * captions_per_video, max_text_words,\n",
    "                                 feat_dim)\n",
    "\n",
    "      input_ids = token_ids[:, :, 0].to(dtype=th.long)\n",
    "      attention_mask = token_ids[:, :, 1].to(dtype=th.long)\n",
    "\n",
    "      word_embs = self.word_embeddings(input_ids)\n",
    "\n",
    "      if self.txt_agg == 'mxp':\n",
    "        # max pooling\n",
    "        word_embs[attention_mask == 0] = -float('Inf')\n",
    "        text = th.max(word_embs, dim=1)[0]\n",
    "\n",
    "      elif self.txt_agg == 'vlad':\n",
    "        text = self.text_pooling(word_embs)\n",
    "\n",
    "      elif self.txt_agg == 'lstm':\n",
    "        x_lengths = get_len_sequences(attention_mask)\n",
    "        text = self.text_pooling(word_embs, x_lengths)\n",
    "\n",
    "    # From the text representation, compute as many embeddings as there are\n",
    "    # modalities\n",
    "    for mod in self.modalities:\n",
    "      layer = self.text_GU[mod]\n",
    "      text_ = layer(text)\n",
    "      text_ = text_.view(b, captions_per_video, -1)\n",
    "      text_embd[mod] = text_\n",
    "    text = text.view(b, captions_per_video, -1)\n",
    "\n",
    "    if self.vid_inp in ['agg', 'both', 'all']:\n",
    "      agg_experts = collections.OrderedDict()\n",
    "      mnp_experts = collections.OrderedDict()\n",
    "      maxp_experts = collections.OrderedDict()\n",
    "\n",
    "      # Embed all features to a common dimension\n",
    "      for mod in self.modalities:\n",
    "        layer = self.video_dim_reduce[mod]\n",
    "        mnp_experts[mod] = layer(pooled_experts[f'{mod}_avgpool'])\n",
    "        maxp_experts[mod] = layer(pooled_experts[f'{mod}_maxpool'])\n",
    "\n",
    "      for mod in self.modalities:\n",
    "        agg_experts[mod] = maxp_experts[mod]\n",
    "\n",
    "    if self.vid_inp in ['both', 'temp', 'all']:\n",
    "      for mod in self.modalities:\n",
    "        layer = self.video_dim_reduce[mod]\n",
    "        experts_feats[mod] = layer(experts_feats[mod])\n",
    "\n",
    "    if self.vid_cont in ['none', 'coll']:\n",
    "      for _, modality in enumerate(self.modalities):\n",
    "        experts[modality] = agg_experts[modality]\n",
    "\n",
    "    # If we use collaborative gating to compute a mask (called T in the\n",
    "    # Collaborative experts paper)\n",
    "    if self.vid_cont == 'coll':\n",
    "      masks = {}\n",
    "      all_combinations = list(itertools.permutations(agg_experts, 2))\n",
    "      assert len(self.modalities) > 1, 'use_ce requires multiple modalities'\n",
    "\n",
    "      for _, modality in enumerate(self.modalities):\n",
    "\n",
    "        mask_num = 0\n",
    "        curr_mask = 0\n",
    "        temp_dict = {}\n",
    "        avai_dict = {}\n",
    "\n",
    "        for modality_pair in all_combinations:\n",
    "          mod0, mod1 = modality_pair\n",
    "          if mod0 == modality:\n",
    "            new_key = '_'.join(modality_pair)\n",
    "            fused = th.cat((agg_experts[mod0], agg_experts[mod1]),\n",
    "                           1)  # -> B x 2D\n",
    "            temp = self.g_reason_1(fused)  # B x 2D -> B x D\n",
    "            temp = self.coll_g_dropout(temp)\n",
    "            temp = self.g_reason_2(F.relu(temp))  # B x D -> B x D\n",
    "            temp_dict[new_key] = temp\n",
    "            avail = (ind[mod0].float() * ind[mod1].float()).to(device)\n",
    "            avai_dict[new_key] = avail\n",
    "\n",
    "        # Combine the paired features into a mask through elementwise summation\n",
    "        for mm in temp_dict:\n",
    "          curr_mask += temp_dict[mm] * avai_dict[mm].unsqueeze(1)\n",
    "          mask_num += avai_dict[mm]\n",
    "\n",
    "        curr_mask = th.div(curr_mask, (mask_num + 0.00000000001).unsqueeze(1))\n",
    "        curr_mask = self.f_reason_1(curr_mask)\n",
    "        curr_mask = self.coll_f_dropout(curr_mask)\n",
    "        curr_mask = self.f_reason_2(F.relu(curr_mask))\n",
    "        masks[modality] = curr_mask\n",
    "\n",
    "        mod_gu = self.video_GU[modality]\n",
    "        experts[modality] = mod_gu(experts[modality], masks[modality])\n",
    "\n",
    "    # If Bert architecture is employed\n",
    "    if self.vid_cont == 'bert':\n",
    "      # 0=[CLS] 1=[SEP] 2=[AGG] 3=[MAXP] 4=[MNP] 5=[VLAD] 6=[FEA]\n",
    "      input_ids_list = []\n",
    "      token_type_ids_list = []  # Modality id\n",
    "      # Position (0 = no position, 1 = unknown, >1 = valid position)\n",
    "      position_ids_list = []\n",
    "      features_list = []  # Semantics\n",
    "      attention_mask_list = []  # Valid token or not\n",
    "\n",
    "      modality_to_tok_map = collections.OrderedDict()\n",
    "\n",
    "      # [CLS] token\n",
    "      tok_id = 0\n",
    "      ids_size = (b,)\n",
    "      input_ids_list.append(th.full(ids_size, 0, dtype=th.long))\n",
    "      token_type_ids_list.append(th.full(ids_size, 0, dtype=th.long))\n",
    "      position_ids_list.append(th.full(ids_size, 0, dtype=th.long).to(device))\n",
    "      features_list.append(\n",
    "          th.full((b, self.same_dim), 0, dtype=th.float).to(device))\n",
    "      attention_mask_list.append(th.full(ids_size, 1, dtype=th.long).to(device))\n",
    "\n",
    "      # Number of temporal tokens per modality\n",
    "      if self.vid_inp in ['temp', 'both', 'all']:\n",
    "        max_expert_tokens = collections.OrderedDict()\n",
    "        for _, modality in enumerate(self.modalities):\n",
    "          max_expert_tokens[modality] = experts_feats[modality].size()[1]\n",
    "\n",
    "      # Make the features_t and raw_captions_t start at the minimum value\n",
    "      if self.pos_enc == 'tint':\n",
    "\n",
    "        # Clamp the position encoding to [0, max_position_embedding - 1]\n",
    "        max_pos = self.vid_bert_params['max_position_embeddings'] - 1\n",
    "        for _, modality in enumerate(self.modalities):\n",
    "          experts_feats_t[modality].clamp_(min=0, max=max_pos)\n",
    "          experts_feats_t[modality] = experts_feats_t[modality].long().to(\n",
    "              device)\n",
    "\n",
    "      for _, modality in enumerate(self.modalities):\n",
    "        token_type = self.expert_dims[modality]['idx']\n",
    "\n",
    "        # Add an aggregated feature token\n",
    "        if self.vid_inp in ['agg', 'both', 'all']:\n",
    "          tok_id += 1\n",
    "          modality_to_tok_map[modality] = tok_id\n",
    "          input_ids_list.append(th.full(ids_size, 2, dtype=th.long))\n",
    "          token_type_ids_list.append(\n",
    "              th.full(ids_size, token_type, dtype=th.long))\n",
    "          position_ids_list.append(\n",
    "              th.full(ids_size, 0, dtype=th.long).to(device))\n",
    "          if self.out_tok == 'sep':\n",
    "            features_list.append(\n",
    "                th.full((b, self.same_dim), 0, dtype=th.float).to(device))\n",
    "          elif self.out_tok == 'mxp':\n",
    "            features_list.append(maxp_experts[modality])\n",
    "          elif self.out_tok == 'mnp':\n",
    "            features_list.append(mnp_experts[modality])\n",
    "          attention_mask_list.append(ind[modality].to(dtype=th.long).to(device))\n",
    "        if self.vid_inp in ['temp', 'both', 'all']:\n",
    "          for frame_id in range(max_expert_tokens[modality]):\n",
    "            if self.pos_enc == 'ordr':\n",
    "              position_ids_list.append(\n",
    "                  th.full(ids_size, frame_id + 1, dtype=th.long).to(device))\n",
    "            elif self.pos_enc == 'tint':\n",
    "              position_ids_list.append(experts_feats_t[modality][:, frame_id])\n",
    "            elif self.pos_enc == 'type':\n",
    "              position_ids_list.append(\n",
    "                  th.full(ids_size, 1, dtype=th.long).to(device))\n",
    "            tok_id += 1\n",
    "            input_ids_list.append(th.full(ids_size, 6, dtype=th.long))\n",
    "            token_type_ids_list.append(\n",
    "                th.full(ids_size, token_type, dtype=th.long))\n",
    "            features_list.append(experts_feats[modality][:, frame_id, :])\n",
    "            attention_mask_list.append(\n",
    "                experts_feats_ind[modality][:, frame_id].to(dtype=th.long))\n",
    "\n",
    "      features = th.stack(features_list, dim=1).to(self.device)\n",
    "      input_ids = th.stack(input_ids_list, dim=1).to(self.device)\n",
    "      token_type_ids = th.stack(token_type_ids_list, dim=1).to(self.device)\n",
    "      if self.pos_enc != 'none':\n",
    "        position_ids = th.stack(position_ids_list, dim=1).to(self.device)\n",
    "      else:\n",
    "        position_ids = None\n",
    "      attention_mask = th.stack(attention_mask_list, dim=1).to(self.device)\n",
    "\n",
    "      if self.debug_dataloader and debug:\n",
    "        token_ids = token_ids.view(b, captions_per_video, max_text_words,\n",
    "                                   feat_dim)\n",
    "        self.display_minibatch(token_ids, input_ids, attention_mask,\n",
    "                               token_type_ids, position_ids, features)\n",
    "        token_ids = token_ids.view(b * captions_per_video, max_text_words,\n",
    "                                   feat_dim)\n",
    "\n",
    "      vid_bert_output = self.vid_bert(input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      position_ids=position_ids,\n",
    "                                      features=features)\n",
    "\n",
    "      last_layer = vid_bert_output[0]\n",
    "      vid_embd = last_layer[:, 0]\n",
    "\n",
    "      for _, modality in enumerate(self.modalities):\n",
    "        experts[modality] = last_layer[:, modality_to_tok_map[modality]]\n",
    "\n",
    "    if self.vid_wgh == 'nrm':\n",
    "      vid_weights = self.compute_weights_from_norm(experts)\n",
    "    elif self.vid_wgh == 'emb':\n",
    "      vid_weights = self.compute_weights_from_emb(vid_embd)\n",
    "    elif self.vid_wgh == 'none':\n",
    "      vid_weights = th.ones(b, m).to(device)\n",
    "    else:\n",
    "      msg = 'video weighting mode {} not supported'\n",
    "      raise NotImplementedError(msg.format(self.vid_wgh))\n",
    "\n",
    "    if not self.keep_missing_modalities:\n",
    "      # Zero padding of the missing modalities\n",
    "      available = th.zeros(b, m).to(device)\n",
    "      for idx, mod in enumerate(self.modalities):\n",
    "        available[:, idx] = ind[mod].float()  # B x M\n",
    "      vid_weights = vid_weights * available\n",
    "\n",
    "    # Normalize video weights so that they sum to one\n",
    "    vid_weights = nn.functional.normalize(vid_weights, p=1, dim=-1)\n",
    "\n",
    "    if self.txt_wgh == 'emb':\n",
    "      text_weights = self.compute_weights_from_emb(text)\n",
    "    elif self.txt_wgh == 'none':\n",
    "      text_weights = th.ones(b, captions_per_video, m).to(device)\n",
    "    else:\n",
    "      msg = 'txt weighting mode {} not supported'\n",
    "      raise NotImplementedError(msg.format(self.txt_wgh))\n",
    "\n",
    "    # Normalize text weights so that they sum to one\n",
    "    text_weights = nn.functional.normalize(text_weights, p=1, dim=-1)\n",
    "\n",
    "    # L2 Normalization of each expert\n",
    "    if self.normalize_experts:\n",
    "      for _, modality in enumerate(self.modalities):\n",
    "        experts[modality] = nn.functional.normalize(experts[modality], dim=-1)\n",
    "        text_embd[modality] = nn.functional.normalize(text_embd[modality],\n",
    "                                                      dim=-1)\n",
    "\n",
    "    if self.training:\n",
    "      merge_caption_similiarities = 'avg'\n",
    "    else:\n",
    "      merge_caption_similiarities = self.test_caption_mode\n",
    "    self.merge_caption_similarities = merge_caption_similiarities\n",
    "\n",
    "    if out == 'conf':  # Output confusion matrix\n",
    "      cross_view_conf_matrix = sharded_cross_view_inner_product(\n",
    "          vid_embds=experts,\n",
    "          text_embds=text_embd,\n",
    "          vid_weights=vid_weights,\n",
    "          text_weights=text_weights,\n",
    "          subspaces=self.modalities,\n",
    "          merge_caption_similiarities=merge_caption_similiarities,\n",
    "      )\n",
    "      return {\n",
    "          'modalities': self.modalities,\n",
    "          'cross_view_conf_matrix': cross_view_conf_matrix,\n",
    "      }\n",
    "    else:  # Output the embeddings\n",
    "      # Transform the dictionaries into tensors\n",
    "      vid_embds_list = []\n",
    "      text_embds_list = []\n",
    "      for idx, mod in enumerate(self.modalities):\n",
    "        vid_embds_list.append(experts[mod].unsqueeze(1))\n",
    "        text_embds_list.append(text_embd[mod].unsqueeze(1))\n",
    "      vid_embds = th.cat(vid_embds_list, 1)\n",
    "      text_embds = th.cat(text_embds_list, 1)\n",
    "\n",
    "      return {\n",
    "          'vid_embds': vid_embds,\n",
    "          'text_embds': text_embds,\n",
    "          'vid_weights': vid_weights,\n",
    "          'text_weights': text_weights,\n",
    "      }\n",
    "\n",
    "  def display_minibatch(self, token_ids, input_ids, attention_mask,\n",
    "                        token_type_ids, position_ids, features):\n",
    "    for i in range(1):\n",
    "      logger.debug()\n",
    "      # logger.debug(f'Sample {i}:')\n",
    "      logger.debug('Text:')\n",
    "      ids = token_ids[i, 0, :, 0].cpu().numpy()\n",
    "      logger.debug(ids)\n",
    "\n",
    "      tokens = self.tokenizer.convert_ids_to_tokens(ids)\n",
    "      logger.debug(tokens)\n",
    "\n",
    "      logger.debug('Video:')\n",
    "      # logger.debug(f'input_ids: {input_ids[i]}')\n",
    "      # logger.debug(f'attention_mask: {attention_mask[i]}')\n",
    "      # logger.debug(f'token_type_ids: {token_type_ids[i]}')\n",
    "      # logger.debug(f'position_ids: {position_ids[i]}')\n",
    "      logger.debug(features[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedEmbeddingUnit(nn.Module):\n",
    "  \"\"\"Gated embedding module.\n",
    "\n",
    "  as described in\n",
    "  \"Learning a Text-Video Embedding from Incomplete and Heterogeneous Data\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_dimension, output_dimension, use_bn, normalize):\n",
    "    super(GatedEmbeddingUnit, self).__init__()\n",
    "\n",
    "    self.fc = nn.Linear(input_dimension, output_dimension)\n",
    "    self.cg = ContextGating(output_dimension, add_batch_norm=use_bn)\n",
    "    self.normalize = normalize\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    x = self.cg(x)\n",
    "    if self.normalize:\n",
    "      x = F.normalize(x, dim=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicCEGatedEmbeddingUnit(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dimension, output_dimension, use_bn):\n",
    "    super().__init__()\n",
    "    self.cg = ContextGating(input_dimension, add_batch_norm=use_bn)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cg(x)\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceDim(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dimension, output_dimension):\n",
    "    super(ReduceDim, self).__init__()\n",
    "    self.fc = nn.Linear(input_dimension, output_dimension)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(nn.Module):\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x = th.cat((x, mask), 1)\n",
    "    return F.glu(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGating(nn.Module):\n",
    "  \"\"\"Context gating class.\"\"\"\n",
    "\n",
    "  def __init__(self, dimension, add_batch_norm=True):\n",
    "    super(ContextGating, self).__init__()\n",
    "    self.fc = nn.Linear(dimension, dimension)\n",
    "    self.add_batch_norm = add_batch_norm\n",
    "    self.batch_norm = nn.BatchNorm1d(dimension)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x1 = self.fc(x)\n",
    "    if self.add_batch_norm:\n",
    "      x1 = self.batch_norm(x1)\n",
    "    x = th.cat((x, x1), 1)\n",
    "    return F.glu(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedEmbeddingUnitReasoning(nn.Module):\n",
    "\n",
    "  def __init__(self, output_dimension):\n",
    "    super(GatedEmbeddingUnitReasoning, self).__init__()\n",
    "    self.cg = ContextGatingReasoning(output_dimension)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x = self.cg(x, mask)\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGatingReasoning(nn.Module):\n",
    "  \"\"\"Context gating reasoning class.\"\"\"\n",
    "\n",
    "  def __init__(self, dimension, add_batch_norm=True):\n",
    "    super(ContextGatingReasoning, self).__init__()\n",
    "    self.fc = nn.Linear(dimension, dimension)\n",
    "    self.add_batch_norm = add_batch_norm\n",
    "    self.batch_norm = nn.BatchNorm1d(dimension)\n",
    "    self.batch_norm2 = nn.BatchNorm1d(dimension)\n",
    "\n",
    "  def forward(self, x, x1):\n",
    "\n",
    "    x2 = self.fc(x)\n",
    "\n",
    "    if self.add_batch_norm:\n",
    "      x1 = self.batch_norm(x1)\n",
    "      x2 = self.batch_norm2(x2)\n",
    "\n",
    "    t = x1 + x2\n",
    "\n",
    "    x = th.cat((x, t), 1)\n",
    "    return F.glu(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharded_cross_view_inner_product(vid_embds,\n",
    "                                     text_embds,\n",
    "                                     vid_weights,\n",
    "                                     text_weights,\n",
    "                                     subspaces,\n",
    "                                     merge_caption_similiarities='avg'):\n",
    "  \"\"\"Compute similarities between all captions and videos.\"\"\"\n",
    "\n",
    "  b = vid_embds[subspaces[0]].size(0)\n",
    "  device = vid_embds[subspaces[0]].device\n",
    "  num_caps = text_embds[subspaces[0]].size(1)\n",
    "  m = len(subspaces)\n",
    "\n",
    "  # unroll separate captions onto first dimension and treat them separately\n",
    "  sims = th.zeros(b * num_caps, b, device=device)\n",
    "\n",
    "  text_weights = text_weights.view(b * num_caps, -1)\n",
    "  vid_weights = vid_weights.view(b, -1)\n",
    "\n",
    "  moe_weights = vid_weights[None, :, :] * text_weights[:, None, :]\n",
    "\n",
    "  norm_weights = th.sum(moe_weights, dim=2)\n",
    "  norm_weights = norm_weights.unsqueeze(2)\n",
    "  # If only one modality is used and is missing in some videos, moe_weights will\n",
    "  # be zero.\n",
    "  # To avoid division by zero, replace zeros by epsilon\n",
    "  # (or anything else, in that case moe_weights are zero anyway)\n",
    "  norm_weights[norm_weights == 0] = 1E-5\n",
    "  moe_weights = th.div(moe_weights, norm_weights)\n",
    "\n",
    "  assert list(moe_weights.size()) == [b * num_caps, b, m]\n",
    "\n",
    "  for idx, mod in enumerate(subspaces):\n",
    "    text_embds[mod] = text_embds[mod].view(b * num_caps, -1)\n",
    "    sims += moe_weights[:, :, idx] * th.matmul(text_embds[mod],\n",
    "                                               vid_embds[mod].t())\n",
    "\n",
    "  if num_caps > 1:\n",
    "    # aggregate similarities from different captions\n",
    "    if merge_caption_similiarities == 'avg':\n",
    "      sims = sims.view(b, num_caps, b)\n",
    "      sims = th.mean(sims, dim=1)\n",
    "      sims = sims.view(b, b)\n",
    "    elif merge_caption_similiarities == 'indep':\n",
    "      pass\n",
    "    else:\n",
    "      msg = 'unrecognised merge mode: {}'\n",
    "      raise ValueError(msg.format(merge_caption_similiarities))\n",
    "  return sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
